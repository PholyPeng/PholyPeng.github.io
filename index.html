<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">

<head>
  <meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
  <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
  <style type="text/css">
    ul,
    li {
      margin-left: 10px;
      margin-right: 10px;
      padding: 0px;
    }

    /* Color scheme stolen from Sergey Karayev */
    a {
      color: #1772d0;
      text-decoration: none;
    }

    a:focus,
    a:hover {
      color: #f09228;
      text-decoration: none;
    }

    body,
    td,
    th,
    tr,
    p,
    a {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px
    }

    .hp-photo {
      width: 240px;
      height: 240px;
      border-radius: 240px;
      -webkit-border-radius: 240px;
      -moz-border-radius: 240px;
    }

    strong {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
    }

    heading {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 24px;
    }

    papertitle {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 15px;
      font-weight: 700;
      color: #0d79e5;
    }

    name {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 32px;
    }

    .one {
      width: 160px;
      height: 160px;
      position: relative;
    }

    .two {
      width: 160px;
      height: 160px;
      position: absolute;
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }

    .fade {
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }

    span.highlight {
      background-color: #ffffd0;
    }
  </style>
  <link rel="stylesheet" href="jemdoc.css" type="text/css" />
  <title>Chensheng Peng</title>
  <link rel="stylesheet" type="text/css" href="/imgs/css">
</head>

<body>
  <div id="layout-content">
    <div id="toptitle">
      <h1>Chensheng Peng</h1>
    </div>
    <table class="imgtable">
      <tr>
        <td>
          <a href="https://PholyPeng.github.io/"><img src="picture/pcs.jpg" alt="alt text" width="120px" /></a>&nbsp;
        </td>
        <td align="left">
          <p>Undergraduate student,<br />
            Artificial Intelligence, Department of Automation, <br />
            Member of <a href="https://en.zhiyuan.sjtu.edu.cn/en/about/overview">Zhiyuan Honors Program</a>, <br />
            <a href="https://www.sjtu.edu.cn/">Shanghai Jiao Tong University (SJTU)</a>, <br />
            Shanghai, 200240, China <br />
            Email: <a href="mailto: pesiter159@gmail.com">pesiter159@gmail.com</a> <br />
            <br />
            <a href="pdf/CV_ChenshengPeng.pdf">[CV]</a> <a
              href="https://scholar.google.com/citations?user=DbZxclcAAAAJ&hl=en&oi=sra">[Google Scholar]</a>
            <a href="https://github.com/PholyPeng">[GitHub]</a>
        </td>
      </tr>
    </table>
    <h2>About me</h2>
    <p>I'm an undergraduate student from Department of Automation, SJTU, studying Artificial Intelligence. </br>
      I worked at <a href="https://irmv.sjtu.edu.cn/"> Intelligent Robotics and Machine Vision (IRMV) Lab</a>, advised by Prof. <a
        href="https://scholar.google.com/citations?user=q6AY9XsAAAAJ&hl">Hesheng Wang</a>, who will be the General Chair of IEEE/RSJ IROS 2025. I also worked with the <a href="https://msc.berkeley.edu/research/autonomous-vehicle.html">Autonomous Driving Group</a> of MSC Lab at UC Berkeley as a research intern, advised by Prof. <a href=https://me.berkeley.edu/people/masayoshi-tomizuka>Masayoshi Tomizuka</a>. </p>

    <h3>Interests:</h3>
    <p> efficient point cloud processing, object detection and tracking, neural architecture search, autonomous driving,
      edge computing</p>

    <h2>Research experiences</h2>


    <table width="100%" align="center" border="0" style="border: none;" cellspacing="0" cellpadding="20">
      <tbody>
        <tr>
          <td width="25%" style="border: none;"><img src="./picture/mot.gif" alt="Tracking" width="225"
              style="border-style: none"></td>
          <td width="75%" valign="top" style="border: none;">
            <p align="left" >
              <papertitle>Interactive Multi-scale Fusion of 2D and 3D Features for Multi-object Tracking</papertitle>
              <br>G. Wang*, <strong>C. Peng*</strong>, J. Zhang, and H. Wang (* indicates equal contributions)</br>
              submitted to IEEE Transactions on Intelligent Transportation Systems (TITS)</br>
              <a href="./pdf/mot.pdf">[Paper]</a><a href="./videos/tracking.MP4">[Video]</a>
            </p>
            <hr style="border:1 dashed #987cb9" width="99%" color=#987cb9 SIZE=1>
            <p align="left" > I join the National College Students' Innovation Program, as a group leader.
            <p align="justify" style="font-size:13px">
            <ul align="left">
              <li align="left" list-style: none;>The research is mainly about multi-object tracking based on
                multi-sensor feature fusion.</li>
              <li align="left">We realize more effective feature fusion by exploring the spatial relation between LiDAR
                point clouds and image pixels.</li>
              <li align="left"> We achieve high accuracy and outperform baselines on the KITTI dataset.</li>
            </ul>
            </p>
            </p>
            <p></p>
          </td>
        </tr>

 

        <!-- <tr>
          <td width="25%" style="border: none;"><img src="./picture/sf.gif" alt="SceneFlow" width="225"
              style="border-style: none"></td>
          <td width="75%" valign="top" style="border: none;">
            <p align="left" >
              <papertitle>Efficient Scene Flow Learning for Large-scale 3D Point Clouds</papertitle></br>
              <strong>C. Peng</strong>, G. Wang, X. Lo, C, Xu. M. Tomizuka, W. Zhan, and H. Wang </br>
              submitted to IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023</br>
              <a href="./videos/seneflow.MP4">[Video]</a>
            </p>
            <hr style="border:1 dashed #987cb9" width="99%" color=#987cb9 SIZE=1>
            <p align="left" > I join the <a href="https://msc.berkeley.edu/research/autonomous-vehicle.html">MSC Lab</a> of UC Berkeley, as a remote research intern.
            <p align="justify" style="font-size:13px">
            <ul align="left">
              <li align="left" list-style: none;>The research is mainly about efficient scene flow prediction from large-scale point clouds</li>
              <li align="left">Our proposed network achieves real-time processing of large-scale point clouds, adopting a dense representation of point clouds and a kernel-based grouping technique.
              </li>
              <li align="left"> The computing time and memory consumption are significantly reduced. </li>
            </ul>
            </p>
            </p>
            <p></p>
          </td>
        </tr> -->

      </tbody>
    </table>

    <div id="footer">
      <div id="footer-text">
        <br>Page generated 2022-09-18, by <a href="https://PholyPeng.github.io/">Chensheng Peng</a>.
      </div>
    </div>
  </div>
</body>

</html>