<!DOCTYPE html>
<!-- saved from url=(0026)https://thomasgultzow.com/ -->
<html lang="en-us">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="viewport" content="width=device-width,initial-scale=1">
  <meta http-equiv="x-ua-compatible" content="IE=edge">
  <meta name="generator" content="Wowchemy 5.7.0 for Hugo">


  <link rel="preconnect" href="https://fonts.gstatic.com/" crossorigin="">

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css"
    integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css"
    integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">
  <!-- <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin="anonymous"> -->
  <script src="https://kit.fontawesome.com/fad964539d.js" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.1.2/lazysizes.min.js"
    integrity="sha256-Md1qLToewPeKjfAHU1zyPwOutccPAm5tahnaw7Osw0A=" crossorigin="anonymous" async></script>
  <link rel="stylesheet"
    href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&display=swap">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/github.min.css"
    crossorigin="anonymous" title="hl-light" disabled>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/dracula.min.css"
    crossorigin="anonymous" title="hl-dark">

  <link rel="preload" as="style" href="css/css2">
  <link rel="stylesheet" href="css/css2" media="all">
  <link rel="stylesheet" href="css/vendor-bundle.min.css" media="all">
  <link rel="stylesheet" href="css/academic.css">
  <link rel="stylesheet" href="css/wowchemy.css">

  <meta name="author" content="Chensheng Peng">
  <meta name="description" content="The personal academic website of Chensheng Peng.">
  <link rel="alternate" hreflang="en-us" href="/">
  <link rel="canonical" href="/">
  <link rel="manifest" href="/index.webmanifest">

  <link rel="icon" type="image/png" href="media/32x32.jpg">
  <link rel="apple-touch-icon" type="image/png" href="media/192x192.jpg">
  <meta name="theme-color" content="#1565c0">
  <!-- <meta property="twitter:card" content="summary">
  <meta property="twitter:site" content="@ThomasGultzow">
  <meta property="twitter:creator" content="@ThomasGultzow">
  <meta property="twitter:image"
    content="media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png"> -->

  <meta property="og:site_name" content="Chensheng Peng">
  <meta property="og:url" content="/">
  <meta property="og:title" content="Chensheng Peng">
  <meta property="og:description" content="The personal academic website of Chensheng Peng.">
  <meta property="og:image" content="media/512x512.jpg">
  <meta property="og:locale" content="en-us">
  <meta property="og:updated_time" content="2024-03-27T14:52:35+00:00">

  <link rel="alternate" href="index.xml" type="application/rss+xml" title="Chensheng Peng">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/highlight.min.js"
    integrity="sha256-1zu+3BnLYV9LdiY85uXMzii3bdrkelyp37e0ZyTAQh0=" crossorigin="anonymous"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/languages/r.min.js"></script>
  <script>const code_highlighting = true;</script>
  <script src="js/academic.min.js"></script>
  <script src="js/netlify-identity-widget.js"></script>

  <title>Home | page of Chensheng Peng</title>
  <style type="text/css">
    .medium-zoom-overlay {
      position: fixed;
      top: 0;
      right: 0;
      bottom: 0;
      left: 0;
      opacity: 0;
      transition: opacity .3s;
      will-change: opacity
    }

    .medium-zoom--opened .medium-zoom-overlay {
      cursor: pointer;
      cursor: zoom-out;
      opacity: 1
    }

    .medium-zoom-image {
      cursor: pointer;
      cursor: zoom-in;
      transition: transform .3s cubic-bezier(.2, 0, .2, 1) !important
    }

    .medium-zoom-image--hidden {
      visibility: hidden
    }

    .medium-zoom-image--opened {
      position: relative;
      cursor: pointer;
      cursor: zoom-out;
      will-change: transform
    }
  </style>
</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#navbar-main" class="page-wrapper dark">
  <script src="./js/wowchemy-init.min.js"></script>
  <aside class="search-modal" id="search">
    <div class="container">
      <section class="search-header">
        <div class="row no-gutters justify-content-between mb-3">
          <div class="col-6">
            <h1>Search</h1>
          </div>
          <div class="col-6 col-search-close"><a class="js-search" href="/#" aria-label="Close"><i
                class="fas fa-times-circle text-muted" aria-hidden="true"></i></a></div>
        </div>
        <div id="search-box"><input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
            autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control"
            aria-label="Search..."></div>
      </section>
      <section class="section-search-results">
        <div id="search-hits"></div>
      </section>
    </div>
  </aside>
  <div class="page-header header--fixed">
    <header>
      <nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
        <div class="container-xl">
          <div class="d-none d-lg-inline-flex">
            <a class="navbar-brand" href="#">Chensheng Peng</a>
          </div>
          <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-content"
            aria-controls="navbar-content" aria-expanded="false" aria-label="Toggle navigation">
            <span><i class="fas fa-bars"></i></span></button>
          <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class="navbar-brand" href="/">Chensheng
              Peng</a></div>
          <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">
            <ul class="navbar-nav d-md-inline-flex">
              <li class="nav-item"><a class="nav-link active" href="#about" data-target="#about"><span>About
                    me</span></a></li>
              <li class="nav-item"><a class="nav-link" href="#papers" data-target="#papers"><span>Research</span></a>
              </li>
              <li class="nav-item"><a class="nav-link" href="#contact" data-target="#contact"><span>Contact</span></a>
              </li>
            </ul>
          </div>

          <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">
            <!-- <li class="nav-item d-none d-lg-inline-flex"><a class="nav-link" href="https://twitter.com/PeterPe86073147"
                target="_blank" rel="noopener" aria-label="twitter"><i class="fab fa-twitter"
                  aria-hidden="true"></i></a></li> -->
            <li class="nav-item"><a class="nav-link js-search" href="#" aria-label="Search"><i class="fas fa-search"
                  aria-hidden="true"></i></a></li>
            <li class="nav-item dropdown theme-dropdown">
              <a href="#" class="nav-link" data-toggle="dropdown" aria-haspopup="true" aria-label="Display preferences">
                <i class="fas fa-circle-half-stroke js-btn-set-theme" aria-hidden="true"></i>
              </a>
              <div class="dropdown-menu">
                <a href="#" class="dropdown-item js-set-theme-light"><span>Light</span></a>
                <a href="#" class="dropdown-item js-set-theme-dark dropdown-item-active"><span>Dark</span></a>
                <a href="#" class="dropdown-item js-set-theme-auto"><span>Automatic</span></a>
              </div>
            </li>
          </ul>
        </div>
      </nav>
    </header>
  </div>

  <div class="page-body"><span class="js-widget-page d-none"></span>
    <section id="about" class="home-section wg-about">
      <div class="home-section-bg"></div>
      <div class="container">
        <div class="row">
          <div class="col-12 col-lg-4">
            <div id="profile"><img class="avatar avatar-circle" width="270" height="270" src="./picture/pic2.jpg"
                alt="csp">
              <div class="portrait-title">
                <h2>Chensheng Peng</h2>
                <h3>PhD Student</h3>
                <h3><a href="https://www.berkeley.edu/" target="_blank" rel="noopener"><span>University of California,
                      Berkeley</span></a>
                </h3>
              </div>
              <ul class="network-icon" aria-hidden="true">
                <li><a href="mailto:chensheng_peng@berkeley.edu" aria-label="envelope"><i
                      class="fas fa-envelope big-icon"></i></a></li>
                <li><a href="https://scholar.google.com/citations?user=DbZxclcAAAAJ" target="_blank" rel="noopener"
                    aria-label="graduation-cap"><i class="fas fa-graduation-cap big-icon"></i></a></li>
                <li><a href="https://www.linkedin.com/in/chensheng-peng-023b34250/" target="_blank" rel="noopener"
                    aria-label="linkedin"><i class="fab fa-linkedin big-icon"></i></a></li>
                <li><a href="https://github.com/PholyPeng" target="_blank" rel="noopener" aria-label="github"><i
                      class="fab fa-github big-icon"></i></a></li>
              </ul>
            </div>
          </div>
          <div class="col-12 col-lg-8">
            <h1>About me</h1>
            <!-- , and Professor Kurt Keutzer -->
            <div class="article-style">
              <p>I am a Ph.D. student at UC Berkeley, working with Professor Masayoshi
                Tomizuka, affiliated with <a href="https://bair.berkeley.edu/" target="_blank" rel="noopener">Berkeley
                  AI Research (BAIR) Lab</a>, <a href="https://deepdrive.berkeley.edu/" target="_blank"
                  rel="noopener">Berkeley DeepDrive (BDD)</a>.</p>
              <p>I received my Bachelor degree from Shanghai Jiao Tong University, where I was an undergrad
                working with Professor Hesheng Wang.</p>

            </div>
            <div class="row">
              <div class="col-md-12">
                <div class="section-subheading">Research Interests</div>
                <ul class="ul-interests mb-0">
                  <li>Generation / Reconstruction
                    <ul class="ul-interests mb-0">
                      <li>3D/4D Generation from Text/Image</li>
                      <li>Gaussian Splatting/NeRF</li>
                      <li>Applications of Diffusion Models</li>
                    </ul>
                  </li>
                  <li>Perception for Autonomous Vehicles
                    <ul class="ul-interests mb-0">
                      <li>Multi-Sensor Fusion</li>
                      <li>Efficient Vision Algorithms</li>
                      <li>Foundation Models</li>
                    </ul>
                  </li>
                </ul>
              </div>

            </div>
          </div>
        </div>
      </div>
    </section>

    <section id="papers" class="home-section wg-collection">
      <div class="home-section-bg"></div>
      <div class="container">
        <div class="center-text">
          <h1 class="mt-0">Publications/Preprints</h1>
        </div>
        <div class="center-text">
          <b>* denotes equal contribution</b>
        </div>
        <div class="row align-items-center justify-content-center">
          <div class="col-auto">
            <div class="btn btn-outline-info btn-page-header btn-lg mx-2 filter-btn active"
              onclick="filterPapers('All', this)">All
            </div>
            <div class="btn btn-outline-info btn-page-header btn-lg filter-btn mx-2" onclick="filterPapers('R', this)">
              Reconstruction
            </div>
            <div class="btn btn-outline-info btn-page-header btn-lg filter-btn mx-2" onclick="filterPapers('G', this)">
              Generation
            </div>
            <div class="btn btn-outline-info btn-page-header btn-lg filter-btn mx-2" onclick="filterPapers('P', this)">
              Perception
            </div>

          </div>
        </div>


        <br>
        <div class="row align-items-center">
          <div class="col-12">

            <!-- DeSiRe-GS -->
            <div class="card-simple view-card paper category-R show">
              <div class="row align-items-center">
                <img class="col-md-6 col-lg-6" src="./project/desiregs.gif" alt="./project/desiregs.png">
                <div class="col-md-6 col-lg-6">

                  <div class="section-subheading article-title mb-1 mt-3"><a
                      href="https://arxiv.org/abs/2411.11921">DeSiRe-GS: 4D Street Gaussians for Static-Dynamic
                      Decomposition and Surface Reconstruction for Urban Driving Scenes</a>
                  </div>
                  <div class="article-metadata">
                    <div>
                      <span class="author-highlighted">Chensheng
                        Peng*</span>, <span>Chengwei Zhang</span>, <span>Yixiao Wang</span>, <span>Chenfeng Xu</span>,
                      <span>Yichen Xie</span>,
                      <span>Wenzhao Zheng</span>, <span>Kurt Keutzer</span>, <span>Masayoshi Tomizuka</span>, <span>Wei
                        Zhan</span>
                    </div>
                    <span class="article-date">2024</span>
                    <span class="middot-divider"></span>
                    <span class="pub-publication">Arxiv</span>
                  </div>

                  <div class="btn-links">
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2411.11921"
                      target="_blank" rel="noopener">Arxiv</a>
                    <a class="btn btn-outline-primary btn-page-header btn-sm"
                      href="https://github.com/chengweialan/DeSiRe-GS" target="_blank" rel="noopener">Code</a>
                  </div>
                  <a href="https://arxiv.org/abs/2411.11921" class="summary-link">
                    <div class="article-style">
                      <p>We present DeSiRe-GS, a self-supervised gaussian splatting representation, enabling effective
                        static-dynamic decomposition and high-fidelity surface reconstruction in complex driving
                        scenarios. Combined with the introduced geometric regularizations, our method are able to
                        address the over-fitting issues
                        caused by data sparsity in autonomous driving, reconstructing physically plausible Gaussians
                        that align with object surfaces rather than floating in air.
                      </p>
                    </div>
                  </a>
                </div>
              </div>
            </div>

            <!-- Diff-GS -->
            <div class="card-simple view-card paper category-G show">
              <div class="row align-items-center">

                <div class="col-md-8 col-lg-8">

                  <div class="section-subheading article-title mb-1 mt-3"><a href="#">A Lesson in Splats: Teacher-Guided
                      Diffusion for 3D Gaussian Splats Generation with 2D Supervision</a></div>
                  <div class="article-metadata">
                    <div>
                      <span class="author-highlighted">Chensheng Peng</span>, <span>Ido Sobol</span>, <span>Masayoshi
                        Tomizuka</span>,
                      <span>Kurt Keutzer</span>, <span>Chenfeng Xu</span>, <span>Or Litany</span>
                    </div>
                    <span class="article-date">2024</span>
                    <span class="middot-divider"></span>
                    <span class="pub-publication">Under review</span>
                  </div>

                  <!-- <div class="btn-links">
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="#"
                      target="_blank" rel="noopener">Arxiv</a>
                    <a class="btn btn-outline-primary btn-page-header btn-sm"
                      href="#" target="_blank" rel="noopener">Website</a>
                  </div> -->
                  <a href="#" class="summary-link">
                    <div class="article-style">
                      <p>We introduce a diffusion model for Gaussian Splats, SplatDiffusion, to enable generation of
                        three-dimensional structures from single images, addressing the ill-posed nature of lifting 2D
                        inputs to 3D. Existing methods rely on deterministic, feed-forward predictions, which limit
                        their ability to handle the inherent ambiguity of 3D inference from 2D data. Diffusion models
                        have recently shown promise as powerful generative models for 3D data, including Gaussian
                        splats; however, standard diffusion frameworks typically require the target signal and denoised
                        signal to be in the same modality, which is challenging given the scarcity of 3D data. To
                        overcome this, we propose a novel training strategy that decouples the denoised modality from
                        the supervision modality. By using a deterministic model as a noisy teacher to create the noised
                        signal and transitioning from single-step to multi-step denoising supervised by an image
                        rendering loss, our approach significantly enhances performance compared to the deterministic
                        teacher. Additionally, our method is flexible, as it can learn from various 3D Gaussian Splat
                        (3DGS) teachers with minimal adaptation; we demonstrate this by surpassing the performance of
                        two different deterministic models as teachers, highlighting the potential generalizability of
                        our framework. Our approach further incorporates a guidance mechanism to aggregate information
                        from multiple views, enhancing reconstruction quality when more than one view is available.
                      </p>
                    </div>
                  </a>
                </div>
                <img class="col-md-4 col-lg-4" src="./project/diffgs.png" alt="./project/diffgs_alt.png">
              </div>
            </div>

            <!-- X-Drive -->
            <div class="card-simple view-card paper category-G show">
              <div class="row align-items-center">
                <img class="col-md-6 col-lg-6" src="./project/xdrive.png" alt="./project/xdrive_alt.png">
                <div class="col-md-6 col-lg-6">

                  <div class="section-subheading article-title mb-1 mt-3"><a
                      href="https://arxiv.org/abs/2411.01123">X-Drive: Cross-modality consistent multi-sensor data
                      synthesis for driving scenarios</a>
                  </div>
                  <div class="article-metadata">
                    <div>
                      <span>Yichen Xie*</span>, <span>Chenfeng Xu*</span>, <span class="author-highlighted">Chensheng
                        Peng</span>,
                      <span>Shuqi Zhao</span>,
                      <span>Nhat Ho</span>, <span>Alexander T. Pham</span>,
                      <span>Mingyu Ding</span>, <span>Masayoshi Tomizuka</span>, <span>Wei Zhan</span>
                    </div>
                    <span class="article-date">2024</span>
                    <span class="middot-divider"></span>
                    <span class="pub-publication">Arxiv</span>
                  </div>

                  <div class="btn-links">
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2411.01123"
                      target="_blank" rel="noopener">Arxiv</a>
                    <a class="btn btn-outline-primary btn-page-header btn-sm"
                      href="https://github.com/yichen928/X-Drive" target="_blank" rel="noopener">Code</a>
                  </div>
                  <a href="https://arxiv.org/abs/2411.01123" class="summary-link">
                    <div class="article-style">
                      <p>We propose a novel framework, X-DRIVE, to model the joint distribution of point clouds and
                        multi-view images via a dual-branch latent diffusion model architecture.
                        Considering the distinct geometrical spaces of the two modalities, X-DRIVE conditions the
                        synthesis of each modality on the corresponding local regions from the other modality, ensuring
                        better alignment and realism.
                        To further handle the spatial ambiguity during denoising, we design the cross-modality condition
                        module based on epipolar lines to adaptively learn the cross-modality local correspondence.
                      </p>
                    </div>
                  </a>
                </div>
              </div>
            </div>

            <!-- CompGS -->
            <div class="card-simple view-card paper category-G show">
              <div class="row align-items-center">

                <div class="col-md-6 col-lg-6">

                  <div class="section-subheading article-title mb-1 mt-3"><a
                      href="https://chongjiange.github.io/compgs.html">CompGS:
                      Unleashing 2D Compositionality for Compositional Text-to-3D via Dynamically Optimizing 3D
                      Gaussians</a></div>
                  <div class="article-metadata">
                    <div>
                      <span>Chongjian Ge</span>, <span>
                        Chenfeng Xu</span>, <span>Yuanfeng Ji</span>, <span class="author-highlighted">Chensheng
                        Peng</span>, <span>Masayoshi Tomizuka</span>, <br> <span> Ping Luo</span>, <span> Mingyu
                        Ding</span>, <span>Wei Zhan</span>, <span>Varun Jampani</span>
                    </div>
                    <span class="article-date">2024</span>
                    <span class="middot-divider"></span>
                    <span class="pub-publication">Arxiv</span>
                  </div>

                  <div class="btn-links">
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2410.20723"
                      target="_blank" rel="noopener">Arxiv</a>
                    <a class="btn btn-outline-primary btn-page-header btn-sm"
                      href="https://chongjiange.github.io/compgs.html" target="_blank" rel="noopener">Website</a>
                  </div>
                  <a href="https://arxiv.org/abs/2410.20723" class="summary-link">
                    <div class="article-style">
                      <p>We introduce CompGS, a novel generative framework that employs 3D Gaussian Splatting (GS) for
                        efficient, compositional text-to-3D content generation. CompGS automatically
                        decomposes 3D Gaussians into distinct entity parts, enabling optimization at both the entity and
                        composition levels.</p>
                    </div>
                  </a>
                </div>
                <img class="col-md-6 col-lg-6" src="./project/compgs.png" alt="./project/compgs_alt.png">
              </div>
            </div>

            <!-- Q-SLAM -->
            <div class="card-simple view-card paper category-R show">
              <div class="row align-items-center">
                <img class="col-md-4 col-lg-4" src="./project/qslam.gif" alt="./project/qslam_alt.png">
                <div class="col-md-8 col-lg-8">

                  <div class="section-subheading article-title mb-1 mt-3"><a
                      href="https://arxiv.org/abs/2403.08125">Q-SLAM: Quadric Representations for Monocular SLAM</a>
                  </div>
                  <div class="article-metadata">
                    <div><span class="author-highlighted">Chensheng Peng*</span>, <span>Chenfeng Xu*</span>, <span>Yue
                        Wang</span>,
                      <span>Mingyu Ding</span>, <span> Heng Yang</span>, <span>Masayoshi Tomizuka</span>,
                      <span>Kurt Keutzer</span>,
                      <span>Marco Pavone</span>, <span>Wei Zhan</span>
                    </div>
                    <span class="article-date">2024</span>
                    <span class="middot-divider"></span>
                    <span class="pub-publication">Conference on Robot Learning (CoRL)</span>
                  </div>

                  <div class="btn-links">
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2403.08125"
                      target="_blank" rel="noopener">Arxiv</a>
                    <!-- <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2403.08125"
                      target="_blank" rel="noopener">Code</a> -->
                  </div>
                  <a href="https://arxiv.org/abs/2403.08125" class="summary-link">
                    <div class="article-style">
                      <p>In this study, we propose a novel
                        approach that reimagines volumetric representations through the lens of quadric forms. We posit
                        that most scene components can be effectively represented as quadric planes. Leveraging this
                        assumption, we reshape the volumetric representations with million of cubes by several quadric
                        planes, which leads to more accurate and efficient modeling of 3D scenes in SLAM contexts.</p>
                    </div>
                  </a>
                </div>
              </div>
            </div>

            <!-- DELFlow -->
            <div class="card-simple view-card paper category-P show">
              <div class="row align-items-center">

                <div class="col-md-6 col-lg-6">

                  <div class="section-subheading article-title mb-1 mt-3"><a
                      href="https://openaccess.thecvf.com/content/ICCV2023/html/Peng_DELFlow_Dense_Efficient_Learning_of_Scene_Flow_for_Large-Scale_Point_ICCV_2023_paper.html">DELFlow:
                      Dense Efficient Learning of Scene Flow for Large-Scale Point Clouds</a></div>
                  <div class="article-metadata">
                    <div><span class="author-highlighted">Chensheng Peng</span>, <span>Guangming Wang</span>, <span>
                        Xian Wan Lo</span>, <span>Xinrui Wu</span>, <span>Chenfeng Xu</span>, <br> <span>Masayoshi
                        Tomizuka</span>, <span>Wei Zhan</span>,<span> Hesheng Wang</span></div>
                    <span class="article-date">2023</span>
                    <span class="middot-divider"></span>
                    <span class="pub-publication">International Conference on Computer Vision (ICCV)</span>
                  </div>

                  <div class="btn-links">
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2308.04383"
                      target="_blank" rel="noopener">Arxiv</a>
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/IRMVLab/DELFlow"
                      target="_blank" rel="noopener">Code</a>
                  </div>
                  <a href="https://openaccess.thecvf.com/content/ICCV2023/html/Peng_DELFlow_Dense_Efficient_Learning_of_Scene_Flow_for_Large-Scale_Point_ICCV_2023_paper.html"
                    class="summary-link">
                    <div class="article-style">
                      <p>Point clouds are naturally sparse, while image pixels are dense. The inconsistency limits
                        feature fusion from both modalities for point-wise scene flow estimation. We regularize raw
                        points to a
                        dense format by storing 3D coordinates in 2D grids. We also present a novel warping
                        projection technique to alleviate the information loss problem.
                        Sufficient experiments demonstrate the efficiency and effectiveness of our method, outperforming
                        the prior-arts on the FlyingThings3D and KITTI dataset.</p>
                    </div>
                  </a>
                </div>
                <img class="col-md-6 col-lg-6" src="./project/deflow.gif" alt="./project/delflow_alt.png">
              </div>
            </div>

            <!-- InterMOT -->
            <div class="card-simple view-card paper category-P show">
              <div class="row align-items-center">
                <img class="col-md-6 col-lg-6" src="./project/intermot.gif" alt="./project/intermot_alt.png">
                <div class="col-md-6 col-lg-6">

                  <div class="section-subheading article-title mb-1 mt-3"><a
                      href="https://ieeexplore.ieee.org/abstract/document/10132881">Interactive
                      multi-scale fusion of 2D and 3D features for multi-object vehicle tracking</a></div>
                  <div class="article-metadata">
                    <div><span class="author-highlighted">Chensheng Peng*</span>, <span>Guangming Wang*</span>, <span>
                        Yingying Gu</span>, <span>Jinpeng Zhang</span>, <span> Hesheng Wang</span></div>
                    <span class="article-date">2023</span>
                    <span class="middot-divider"></span>
                    <span class="pub-publication">IEEE Transactions on Intelligent Transportation Systems</span>
                  </div>

                  <div class="btn-links">
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2203.16268"
                      target="_blank" rel="noopener">Arxiv</a>
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/IRMVLab/InterMOT"
                      target="_blank" rel="noopener">Code</a>
                  </div>
                  <a href="https://ieeexplore.ieee.org/abstract/document/10132881" class="summary-link">
                    <div class="article-style">
                      <p>In this paper, we propose multi-scale interactive query and fusion between pixel-wise and
                        point-wise features to
                        obtain more discriminative features. In addition, an attention mechanism is utilized to conduct
                        soft feature fusion between multiple pixels and points to avoid inaccurate match problems of
                        previous single pixel-point fusion methods. Our method can achieve 90.32% MOTA and 72.44% HOTA
                        on the KITTI benchmark
                        and outperform other approaches without using multi-scale soft feature fusion.</p>
                    </div>
                  </a>
                </div>
              </div>
            </div>


            <!-- PNAS-MOT -->
            <div class="card-simple view-card paper category-P show">
              <div class="row align-items-center">

                <div class="col-md-6 col-lg-6">

                  <div class="section-subheading article-title mb-1 mt-3"><a
                      href="https://ieeexplore.ieee.org/document/10476688">
                      PNAS-MOT: Multi-Modal Object Tracking with Pareto Neural Architecture Search</a></div>
                  <div class="article-metadata">
                    <div><span class="author-highlighted">Chensheng Peng</span>, <span>Zhaoyu Zeng</span>, <span>
                        Jinling Gao</span>, <span> Jundong Zhou</span>, <span>Masayoshi Tomizuka</span>, <span>Xinbing
                        Wang</span>,
                      <span>Chenghu Zhou</span>, <span>Nanyang Ye</span>
                    </div>
                    <span class="article-date">2024</span>
                    <span class="middot-divider"></span>
                    <span class="pub-publication">IEEE Robotics and Automation Letters (RAL)</span>
                  </div>

                  <div class="btn-links">
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2403.15712"
                      target="_blank" rel="noopener">Arxiv</a>
                    <a class="btn btn-outline-primary btn-page-header btn-sm"
                      href="https://github.com/PholyPeng/PNAS-MOT" target="_blank" rel="noopener">Code</a>
                  </div>
                  <a href="https://ieeexplore.ieee.org/document/10476688" class="summary-link">
                    <div class="article-style">
                      <p>In this paper,
                        we explore the use of the neural architecture search (NAS) methods to search for efficient
                        architectures for tracking, aiming for low real-time latency while maintaining relatively high
                        accuracy. We also propose a multi-modal framework to improve the robustness. Experiments
                        demonstrate
                        that our algorithm can run on edge devices within lower latency constraints, thus greatly
                        reducing the computational requirements for multi-modal object tracking while keeping lower
                        latency.
                      </p>
                    </div>
                  </a>
                </div>
                <img class="col-md-6 col-lg-6" src="./project/pnasmot.gif" alt="./project/pnasmot_alt.png">
              </div>
            </div>

          </div>
        </div>

        <div class="center-text mt-3">
          <a href="https://scholar.google.com/citations?user=DbZxclcAAAAJ">See all publications<i
              class="fas fa-angle-right"></i></a>
        </div>

      </div>


    </section>

    <section id="contact" class="home-section wg-tag-cloud">
      <div class="home-section-bg"></div>
      <div class="container">
        <div class="row">
          <div
            class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start">
            <h1 class="mb-0">Contact</h1>
          </div>
          <div class="col-12 col-lg-8">
            <ul>
              <li>Email: chensheng_peng [AT] berkeley [DOT] edu</li>
            </ul>
            <ul class="fa-ul"></ul>
          </div>
        </div>
      </div>
    </section>
  </div>
  <div class="page-footer">
    <div class="container">
      <footer class="site-footer">
        <p class="powered-by copyright-license-text">© 2024 Me. This work is licensed under <a
            href="https://creativecommons.org/licenses/by-nc-nd/4.0" rel="noopener noreferrer" target="_blank">CC BY NC
            ND 4.0</a></p>
        <p class="powered-by footer-license-icons"><a href="https://creativecommons.org/licenses/by-nc-nd/4.0"
            rel="noopener noreferrer" target="_blank" aria-label="Creative Commons"><i
              class="fab fa-creative-commons fa-2x" aria-hidden="true"></i>
            <i class="fab fa-creative-commons-by fa-2x" aria-hidden="true"></i>
            <i class="fab fa-creative-commons-nc fa-2x" aria-hidden="true"></i>
            <i class="fab fa-creative-commons-nd fa-2x" aria-hidden="true"></i></a></p>
        <p class="powered-by">Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target="_blank"
            rel="noopener">Wowchemy</a> — the free, <a href="https://github.com/wowchemy/wowchemy-hugo-themes"
            target="_blank" rel="noopener">open source</a> website builder that empowers creators.</p>
      </footer>
    </div>
  </div>

  <script src="js/vendor-bundle.min.js"></script>
  <script id="search-hit-fuse-template" type="text/x-template">
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script>
  <script src="js/fuse.min.js"
    integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw=="
    crossorigin="anonymous"></script>
  <script src="js/jquery.mark.min.js"
    integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg=="
    crossorigin="anonymous"></script>
  <script id="page-data" type="application/json">{"use_headroom":false}</script>
  <script src="js/wowchemy.min.js"></script>
  <script src="js/wowchemy-publication.js" type="module"></script>
  <script>
    function filterPapers(category, button) {
      const papers = document.querySelectorAll('.paper');
      const buttons = document.querySelectorAll('.filter-btn');

      // Update paper visibility
      papers.forEach(paper => {
        if (category === 'All') {
          paper.classList.add('show');
        } else {
          if (paper.classList.contains(`category-${category}`)) {
            paper.classList.add('show');
          } else {
            paper.classList.remove('show');
          }
        }
      });

      // Remove the 'active' class from all buttons
      buttons.forEach(btn => btn.classList.remove('active'));

      // Add the 'active' class to the clicked button
      button.classList.add('active');
    }
  </script>

  <style>
    .paper {
      display: none;
    }

    .paper.show {
      display: block;
    }
  </style>
</body>

</html>