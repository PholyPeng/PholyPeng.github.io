<!DOCTYPE html>
<!-- saved from url=(0026)https://thomasgultzow.com/ -->
<html lang="en-us">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="viewport" content="width=device-width,initial-scale=1">
  <meta http-equiv="x-ua-compatible" content="IE=edge">
  <meta name="generator" content="Wowchemy 5.7.0 for Hugo">


  <link rel="preconnect" href="https://fonts.gstatic.com/" crossorigin="">

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css"
    integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css"
    integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">
  <!-- <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin="anonymous"> -->
  <script src="https://kit.fontawesome.com/fad964539d.js" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.1.2/lazysizes.min.js"
    integrity="sha256-Md1qLToewPeKjfAHU1zyPwOutccPAm5tahnaw7Osw0A=" crossorigin="anonymous" async></script>
  <link rel="stylesheet"
    href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&display=swap">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/github.min.css"
    crossorigin="anonymous" title="hl-light" disabled>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/dracula.min.css"
    crossorigin="anonymous" title="hl-dark">

  <link rel="preload" as="style" href="css/css2">
  <link rel="stylesheet" href="css/css2" media="all">
  <link rel="stylesheet" href="css/vendor-bundle.min.css" media="all">
  <link rel="stylesheet" href="css/academic.css">
  <link rel="stylesheet" href="css/wowchemy.css">

  <meta name="author" content="Chensheng Peng">
  <meta name="description" content="The personal academic website of Chensheng Peng.">
  <link rel="alternate" hreflang="en-us" href="/">
  <link rel="canonical" href="/">
  <link rel="manifest" href="/index.webmanifest">

  <link rel="icon" type="image/png"
    href="media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_32x32_fill_lanczos_center_2.png">
  <link rel="apple-touch-icon" type="image/png"
    href="media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_192x192_fill_lanczos_center_2.png">
  <meta name="theme-color" content="#1565c0">
  <!-- <meta property="twitter:card" content="summary">
  <meta property="twitter:site" content="@ThomasGultzow">
  <meta property="twitter:creator" content="@ThomasGultzow">
  <meta property="twitter:image"
    content="media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png"> -->

  <meta property="og:site_name" content="Chensheng Peng">
  <meta property="og:url" content="/">
  <meta property="og:title" content="Chensheng Peng">
  <meta property="og:description" content="The personal academic website of Chensheng Peng.">
  <meta property="og:image"
    content="media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png">
  <meta property="og:locale" content="en-us">
  <meta property="og:updated_time" content="2024-03-27T14:52:35+00:00">

  <link rel="alternate" href="index.xml" type="application/rss+xml" title="Chensheng Peng">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/highlight.min.js"
    integrity="sha256-1zu+3BnLYV9LdiY85uXMzii3bdrkelyp37e0ZyTAQh0=" crossorigin="anonymous"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/languages/r.min.js"></script>
  <script>const code_highlighting = true;</script>
  <script src="/js/academic.min.e5c8525332f417fe3589df9a6b25b6c4.js"></script>
  <script src="/js/netlify-identity-widget.js"></script>

  <title>Home | page of Chensheng Peng</title>
  <style type="text/css">
    .medium-zoom-overlay {
      position: fixed;
      top: 0;
      right: 0;
      bottom: 0;
      left: 0;
      opacity: 0;
      transition: opacity .3s;
      will-change: opacity
    }

    .medium-zoom--opened .medium-zoom-overlay {
      cursor: pointer;
      cursor: zoom-out;
      opacity: 1
    }

    .medium-zoom-image {
      cursor: pointer;
      cursor: zoom-in;
      transition: transform .3s cubic-bezier(.2, 0, .2, 1) !important
    }

    .medium-zoom-image--hidden {
      visibility: hidden
    }

    .medium-zoom-image--opened {
      position: relative;
      cursor: pointer;
      cursor: zoom-out;
      will-change: transform
    }
  </style>
</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#navbar-main" class="page-wrapper dark">
  <script src="./js/wowchemy-init.min.js"></script>
  <aside class="search-modal" id="search">
    <div class="container">
      <section class="search-header">
        <div class="row no-gutters justify-content-between mb-3">
          <div class="col-6">
            <h1>Search</h1>
          </div>
          <div class="col-6 col-search-close"><a class="js-search" href="/#" aria-label="Close"><i
                class="fas fa-times-circle text-muted" aria-hidden="true"></i></a></div>
        </div>
        <div id="search-box"><input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
            autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control"
            aria-label="Search..."></div>
      </section>
      <section class="section-search-results">
        <div id="search-hits"></div>
      </section>
    </div>
  </aside>
  <div class="page-header header--fixed">
    <header>
      <nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
        <div class="container-xl">
          <div class="d-none d-lg-inline-flex">
            <a class="navbar-brand" href="/">Chensheng Peng</a>
          </div>
          <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-content"
            aria-controls="navbar-content" aria-expanded="false" aria-label="Toggle navigation">
            <span><i class="fas fa-bars"></i></span></button>
          <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class="navbar-brand" href="/">Chensheng
              Peng</a></div>
          <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">
            <ul class="navbar-nav d-md-inline-flex">
              <li class="nav-item"><a class="nav-link active" href="#about" data-target="#about"><span>About
                    me</span></a></li>
              <li class="nav-item"><a class="nav-link" href="#featured"
                  data-target="#featured"><span>Research</span></a></li>
              <li class="nav-item"><a class="nav-link" href="#contact" data-target="#contact"><span>Contact</span></a>
              </li>
            </ul>
          </div>

          <!-- <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">
            <li class="nav-item d-none d-lg-inline-flex"><a class="nav-link" href="https://twitter.com/PeterPe86073147"
                target="_blank" rel="noopener" aria-label="twitter"><i class="fab fa-twitter"
                  aria-hidden="true"></i></a></li>
            <li class="nav-item"><a class="nav-link js-search" href="#" aria-label="Search"><i class="fas fa-search"
                  aria-hidden="true"></i></a></li>
            <li class="nav-item dropdown theme-dropdown"><a href="#" class="nav-link" data-toggle="dropdown"
                aria-haspopup="true" aria-label="Display preferences"><i class="fas fa-moon" aria-hidden="true"></i></a>
              <div class="dropdown-menu"><a href="#" class="dropdown-item js-set-theme-light"><span>Light</span></a>
                <a href="#" class="dropdown-item js-set-theme-dark"><span>Dark</span></a>
                <a href="#" class="dropdown-item js-set-theme-auto dropdown-item-active"><span>Automatic</span></a>
              </div>
            </li>
          </ul> -->
        </div>
      </nav>
    </header>
  </div>

  <div class="page-body"><span class="js-widget-page d-none"></span>
    <section id="about" class="home-section wg-about">
      <div class="home-section-bg"></div>
      <div class="container">
        <div class="row">
          <div class="col-12 col-lg-4">
            <div id="profile"><img class="avatar avatar-circle" width="270" height="270" src="./picture/pic.jpg"
                alt="csp">
              <div class="portrait-title">
                <h2>Chensheng Peng</h2>
                <h3>PhD Student</h3>
                <h3><a href="https://www.berkeley.edu/" target="_blank" rel="noopener"><span>University of California,
                      Berkeley</span></a>
                </h3>
              </div>
              <ul class="network-icon" aria-hidden="true">
                <li><a href="mailto:chensheng_peng@berkeley.edu" aria-label="envelope"><i
                      class="fas fa-envelope big-icon"></i></a></li>
                <li><a href="https://scholar.google.com/citations?user=DbZxclcAAAAJ" target="_blank" rel="noopener"
                    aria-label="graduation-cap"><i class="fas fa-graduation-cap big-icon"></i></a></li>
                <li><a href="https://www.linkedin.com/in/chensheng-peng-023b34250/" target="_blank" rel="noopener"
                    aria-label="linkedin"><i class="fab fa-linkedin big-icon"></i></a></li>
                <li><a href="https://github.com/PholyPeng" target="_blank" rel="noopener" aria-label="github"><i
                      class="fab fa-github big-icon"></i></a></li>
              </ul>
            </div>
          </div>
          <div class="col-12 col-lg-8">
            <h1>About me</h1>
            <div class="article-style">
              <p>I am a Ph.D. student at UC Berkeley, working with Professor <a
                  href="https://scholar.google.com/citations?user=8m8taGEAAAAJ" target="_blank" rel="noopener">Masayoshi
                  Tomizuka</a>, affiliated with <a href="https://bair.berkeley.edu/" target="_blank"
                  rel="noopener">Berkeley AI Research (BAIR)</a>, <a href="https://deepdrive.berkeley.edu/"
                  target="_blank" rel="noopener">Berkeley DeepDrive (BDD)</a> and <a href="https://msc.berkeley.edu/"
                  target="_blank" rel="noopener">MSC lab.</a></p>
              <p>I received my Bachelor degree from Shanghai Jiao Tong University, where I was a happy undergrad
                working with Professor <a href="https://irmv.sjtu.edu.cn/team/" target="_blank" rel="noopener"> Hesheng
                  Wang</a>.</p>
              <!-- <p>I am also currently the chair of the <a href="https://ehps.net/digital-health-and-computer-tailoring/"
                  target="_blank" rel="noopener">Digital Health &amp; Computer-Tailoring Special Interest Group
                  (SIG)</a>üë®‚Äçüíª of the <a href="https://ehps.net/" target="_blank" rel="noopener">European Health
                  Psychology Society (EHPS)</a> and editor-in-chief of the <a
                  href="https://www.ehps.net/ehp/index.php/contents/index" target="_blank" rel="noopener">European
                  Health Psychologist</a> üì∞</p> -->
            </div>
            <div class="row">
              <div class="col-md-12">
                <div class="section-subheading">Research Interests</div>
                <ul class="ul-interests mb-0">
                  <li>Perception for Autonomous Vehicles
                    <ul class="ul-interests mb-0">
                      <li>Multi-Sensor Fusion</li>
                      <li>Efficient Vision Applications</li>
                      <li>Object Detection and Tracking</li>
                    </ul>
                  </li>
                  <li>Generative AI
                    <ul class="ul-interests mb-0">
                      <li>3D Generation from Text/Image</li>
                      <li>Applications of Diffusion Models</li>
                    </ul>
                  </li>
                </ul>
              </div>
              <!-- <div class="col-md-7">
                <div class="section-subheading">Education</div>
                <ul class="ul-edu fa-ul mb-0">
                  <li><i class="fa-li fas fa-graduation-cap"></i>
                    <div class="description">
                      <p class="course">PhD in Health Promotion, 2022</p>
                      <p class="institution">Maastricht University</p>
                    </div>
                  </li>
                  <li><i class="fa-li fas fa-graduation-cap"></i>
                    <div class="description">
                      <p class="course">Master of Science in Health Education and Promotion, 2017</p>
                      <p class="institution">Maastricht University</p>
                    </div>
                  </li>
                  <li><i class="fa-li fas fa-graduation-cap"></i>
                    <div class="description">
                      <p class="course">Bachelor of Science in Nursing, 2014</p>
                      <p class="institution">Catholic University of Applied Sciences of North Rhine-Westphalia</p>
                    </div>
                  </li>
                </ul>
              </div> -->
            </div>
          </div>
        </div>
      </div>
    </section>
    <!-- <section id="teaching" class="home-section wg-collection">
      <div class="home-section-bg"></div>
      <div class="container">
        <div class="row">
          <div
            class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start">
            <h1 class="mb-0">Teaching</h1>
          </div>
          <div class="col-12 col-lg-8">
            <p>Currently I am co-examiner of a course focused on qualitative research and review reports within other
              method-focused courses. I also have (among other things) teaching experience in:</p>
            <ul>
              <li>Development, implementation, and evaluation of (behavior change) interventions</li>
              <li>Health promotion</li>
              <li>Behavior (change) theories</li>
              <li>Self-regulation</li>
              <li>Research methods</li>
              <li>Anxiety disorders</li>
              <li>Thesis supervision</li>
            </ul>
            <p>In 2021, I obtained my Basiskwalificatie Onderwijs (BKO) / Dutch University Teaching Qualification (UTQ).
            </p>
          </div>
        </div>
      </div>
    </section> -->
    <section id="featured" class="home-section wg-collection">
      <div class="home-section-bg"></div>
      <div class="container">
        <div class="center-text">
          <h1 class="mt-0">Publications/Preprints</h1>
        </div>
        <div class="center-text">
          <b>* denotes equal contribution</b>
        </div><br>
        <div class="row align-items-center">
          <div class="col-12 col-md-6 col-lg-12">

            <div class="card-simple view-card">
              <div class="row align-items-center">
                <img class="col-12 col-md-3 col-lg-6" src="./picture/mot.gif" alt="1">
                <div class="col-12 col-md-3 col-lg-6">

                  <div class="section-subheading article-title mb-1 mt-3"><a
                      href="https://arxiv.org/abs/2403.08125">Q-SLAM: Quadric Representations for Monocular SLAM</a>
                  </div>
                  <div class="article-metadata">
                    <div><span class="author-highlighted">Chensheng Peng</span>, <span>Chenfeng Xu</span>, <span>Yue
                        Wang</span>,
                      <span>Mingyu Ding</span>, <span> Heng Yang</span>, <br> <span>Masayoshi Tomizuka</span>,
                      <span>Kurt Keutzer</span>,
                      <span>Marco Pavone</span>, <span>Wei Zhan</span>
                    </div>
                    <span class="article-date">2024</span>
                    <span class="middot-divider"></span>
                    <span class="pub-publication">Arxiv Preprint</span>
                  </div>

                  <div class="btn-links">
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2403.08125"
                      target="_blank" rel="noopener">Arxiv</a>
                    <!-- <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2403.08125"
                      target="_blank" rel="noopener">Code</a> -->
                  </div>
                  <a href="https://arxiv.org/abs/2403.08125" class="summary-link">
                    <div class="article-style">
                      <p>In this paper, we propose multi-scale interactive query and fusion between pixel-wise and
                        point-wise features to
                        obtain more discriminative features. In addition, an attention mechanism is utilized to conduct
                        soft feature fusion between multiple pixels and points to avoid inaccurate match problems of
                        previous single pixel-point fusion methods. Our method can achieve 90.32% MOTA and 72.44% HOTA
                        on the KITTI benchmark
                        and outperform other approaches without using multi-scale soft feature fusion.</p>
                    </div>
                  </a>
                </div>
              </div>
            </div>


            <div class="card-simple view-card">
              <div class="row align-items-center">

                <div class="col-12 col-md-3 col-lg-6">

                  <div class="section-subheading article-title mb-1 mt-3"><a
                      href="https://openaccess.thecvf.com/content/ICCV2023/html/Peng_DELFlow_Dense_Efficient_Learning_of_Scene_Flow_for_Large-Scale_Point_ICCV_2023_paper.html">DELFlow:
                      Dense Efficient Learning of Scene Flow for Large-Scale Point Clouds</a></div>
                  <div class="article-metadata">
                    <div><span class="author-highlighted">Chensheng Peng</span>, <span>Guangming Wang</span>, <span>
                        Xian Wan Lo</span>, <span>Xinrui Wu</span>, <span>Chenfeng Xu</span>, <br> <span>Masayoshi
                        Tomizuka</span>, <span>Wei Zhan</span>,<span> Hesheng Wang</span></div>
                    <span class="article-date">2023</span>
                    <span class="middot-divider"></span>
                    <span class="pub-publication">International Conference on Computer Vision (ICCV)</span>
                  </div>

                  <div class="btn-links">
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2308.04383"
                      target="_blank" rel="noopener">Arxiv</a>
                    <!-- <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/IRMVLab/DELFlow"
                      target="_blank" rel="noopener">Code</a> -->
                  </div>
                  <a href="https://openaccess.thecvf.com/content/ICCV2023/html/Peng_DELFlow_Dense_Efficient_Learning_of_Scene_Flow_for_Large-Scale_Point_ICCV_2023_paper.html"
                    class="summary-link">
                    <div class="article-style">
                      <p>Point clouds are naturally sparse, while image pixels are dense. The inconsistency limits
                        feature fusion from both modalities for point-wise scene flow estimation. We regularize raw
                        points to a
                        dense format by storing 3D coordinates in 2D grids. We also present a novel warping
                        projection technique to alleviate the information loss problem.
                        Sufficient experiments demonstrate the efficiency and effectiveness of our method, outperforming
                        the prior-arts on the FlyingThings3D and KITTI dataset.</p>
                    </div>
                  </a>
                </div>
                <img class="col-12 col-md-3 col-lg-6" src="./picture/mot.gif" alt="2">
              </div>
            </div>


            <div class="card-simple view-card">
              <div class="row align-items-center">
                <img class="col-12 col-md-3 col-lg-6" src="./picture/mot.gif" alt="1">
                <div class="col-12 col-md-3 col-lg-6">

                  <div class="section-subheading article-title mb-1 mt-3"><a
                      href="https://ieeexplore.ieee.org/abstract/document/10132881">Interactive
                      multi-scale fusion of 2D and 3D features for multi-object vehicle tracking</a></div>
                  <div class="article-metadata">
                    <div><span class="author-highlighted">Chensheng Peng*</span>, <span>Guangming Wang*</span>, <span>
                        Yingying Gu</span>, <span>Jinpeng Zhang</span>, <span> Hesheng Wang</span></div>
                    <span class="article-date">2023</span>
                    <span class="middot-divider"></span>
                    <span class="pub-publication">IEEE Transactions on Intelligent Transportation Systems</span>
                  </div>

                  <div class="btn-links">
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2203.16268"
                      target="_blank" rel="noopener">Arxiv</a>
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/IRMVLab/InterMOT"
                      target="_blank" rel="noopener">Code</a>
                  </div>
                  <a href="https://ieeexplore.ieee.org/abstract/document/10132881" class="summary-link">
                    <div class="article-style">
                      <p>In this paper, we propose multi-scale interactive query and fusion between pixel-wise and
                        point-wise features to
                        obtain more discriminative features. In addition, an attention mechanism is utilized to conduct
                        soft feature fusion between multiple pixels and points to avoid inaccurate match problems of
                        previous single pixel-point fusion methods. Our method can achieve 90.32% MOTA and 72.44% HOTA
                        on the KITTI benchmark
                        and outperform other approaches without using multi-scale soft feature fusion.</p>
                    </div>
                  </a>
                </div>
              </div>
            </div>



            <div class="card-simple view-card">
              <div class="row align-items-center">

                <div class="col-12 col-md-3 col-lg-6">

                  <div class="section-subheading article-title mb-1 mt-3"><a
                      href="https://ieeexplore.ieee.org/document/10476688">
                      PNAS-MOT: Multi-Modal Object Tracking with Pareto Neural Architecture Search</a></div>
                  <div class="article-metadata">
                    <div><span class="author-highlighted">Chensheng Peng</span>, <span>Zhaoyu Zeng</span>, <span>
                        Jinling Gao</span>, <span> Jundong Zhou</span>, <span>Masayoshi Tomizuka</span>, <span>Xinbing
                        Wang</span>,
                      <span>Chenghu Zhou</span>, <span>Nanyang Ye</span>
                    </div>
                    <span class="article-date">2024</span>
                    <span class="middot-divider"></span>
                    <span class="pub-publication">IEEE Robotics and Automation Letters (RAL)</span>
                  </div>

                  <div class="btn-links">
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2403.15712"
                      target="_blank" rel="noopener">Arxiv</a>
                    <a class="btn btn-outline-primary btn-page-header btn-sm"
                      href="https://github.com/PholyPeng/PNAS-MOT" target="_blank" rel="noopener">Code</a>
                  </div>
                  <a href="https://ieeexplore.ieee.org/document/10476688" class="summary-link">
                    <div class="article-style">
                      <p>In this paper,
                        we explore the use of the neural architecture search (NAS) methods to search for efficient
                        architectures for tracking, aiming for low real-time latency while maintaining relatively high
                        accuracy. We also propose a multi-modal framework to improve the robustness. Experiments
                        demonstrate
                        that our algorithm can run on edge devices within lower latency constraints, thus greatly
                        reducing the computational requirements for multi-modal object tracking while keeping lower
                        latency.
                      </p>
                    </div>
                  </a>
                </div>
                <img class="col-12 col-md-3 col-lg-6" src="./picture/mot.gif" alt="2">
              </div>
            </div>

          </div>
        </div>

        <div class="center-text mt-3">
          <a href="https://scholar.google.com/citations?user=DbZxclcAAAAJ">See all publications<i
              class="fas fa-angle-right"></i></a>
        </div>

      </div>


    </section>


    <!-- <section id="publication" class="home-section wg-collection">
      <div class="home-section-bg"></div>
      <div class="container">
        <div class="row">
          <div
            class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start">
            <h1 class="mb-0">Recent Publications</h1>
          </div>
          <div class="col-12 col-lg-8">
            <div class="alert alert-note">
              <div>Quickly discover relevant content by <a href="https://thomasgultzow.com/publication/">filtering
                  publications</a>.</div>
            </div>
            <div class="pub-list-item view-citation" style="margin-bottom:1rem"><i class="far fa-file-alt pub-icon"
                aria-hidden="true"></i>
              <span class="article-metadata li-cite-author"><span>Laura M. K√∂nig</span>, <span>Anila Allmeta</span>,
                <span>Olga Perski</span>, <span>Eline Suzanne Smit</span>, <span>Katie Newby</span>, <span>Corneel
                  Vandelanotte</span>, <span>Shoba Poduval</span>, <span>Lorenzo Gordon</span>, <span
                  class="author-highlighted">Thomas G√ºltzow</span>, <span>Monique C. Alblas</span>, <span>Emily
                  Arden-Close</span>, <span>Maya Braun</span>, <span>Klara Greffin</span>, <span>Rachel Hewitt</span>,
                <span>Liam Knox</span>, <span>Claire McCallum</span>, <span>Thomas McLaren</span>, <span>Dorothy
                  Szinay</span>, <span>Carmen Peuters</span>, <span>Ann DeSmet</span></span>
              (2024).
              <a
                href="https://thomasgultzow.com/publication/towards-meaningful-interdisciplinary-and-cross-sectoral-digital-health-collaborations-challenges-and-action-oriented-solutions/">Towards
                meaningful interdisciplinary and cross-sectoral digital health collaborations: Challenges and
                action-oriented solutions</a>.
              OSF Preprints.<p><a class="btn btn-outline-primary btn-page-header btn-sm"
                  href="https://doi.org/https://doi.org/10.31219/osf.io/d9zx7" target="_blank" rel="noopener">DOI</a>
              </p>
            </div>
            <div class="pub-list-item view-citation" style="margin-bottom:1rem"><i class="far fa-file-alt pub-icon"
                aria-hidden="true"></i>
              <span class="article-metadata li-cite-author"><span>Filipa Teixeira</span>, <span
                  class="author-highlighted">Thomas G√ºltzow</span></span>
              (2024).
              <a href="https://thomasgultzow.com/publication/to-new-beginnings/">To new beginnings</a>.
              The European Health Psychologist.<p></p>
            </div>
            <div class="pub-list-item view-citation" style="margin-bottom:1rem"><i class="far fa-file-alt pub-icon"
                aria-hidden="true"></i>
              <span class="article-metadata li-cite-author"><span>Haoyi Wang</span>, <span>Kennedy J. I. D‚Äôabreu de
                  Paulo</span>, <span class="author-highlighted">Thomas G√ºltzow</span>, <span>Hanne M. L.
                  Zimmermann</span>, <span>Kai J. Jonas</span></span>
              (2024).
              <a
                href="https://thomasgultzow.com/publication/brief-report-determinants-of-potential-sexual-activity-reduction-in-the-face-of-the-monkeypox-epidemic/">Brief
                report: Determinants of potential sexual activity reduction in the face of the Monkeypox epidemic</a>.
              International Journal of Behavioral Medicine.<p><a class="btn btn-outline-primary btn-page-header btn-sm"
                  href="https://doi.org/https://doi.org/10.1007/s12529-023-10252-4" target="_blank"
                  rel="noopener">DOI</a></p>
            </div>
            <div class="see-all"><a href="https://thomasgultzow.com/publication/">See all publications
                <i class="fas fa-angle-right"></i></a></div>
          </div>
        </div>
      </div>
    </section> -->
    <section id="contact" class="home-section wg-tag-cloud">
      <div class="home-section-bg"></div>
      <div class="container">
        <div class="row">
          <div
            class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start">
            <h1 class="mb-0">Contact</h1>
          </div>
          <div class="col-12 col-lg-8">
            <ul>
              <li>Email: chensheng_peng [AT] berkeley [DOT] edu</li>
            </ul>
            <ul class="fa-ul"></ul>
          </div>
        </div>
      </div>
    </section>
  </div>
  <div class="page-footer">
    <div class="container">
      <footer class="site-footer">
        <p class="powered-by copyright-license-text">¬© 2024 Me. This work is licensed under <a
            href="https://creativecommons.org/licenses/by-nc-nd/4.0" rel="noopener noreferrer" target="_blank">CC BY NC
            ND 4.0</a></p>
        <p class="powered-by footer-license-icons"><a href="https://creativecommons.org/licenses/by-nc-nd/4.0"
            rel="noopener noreferrer" target="_blank" aria-label="Creative Commons"><i
              class="fab fa-creative-commons fa-2x" aria-hidden="true"></i>
            <i class="fab fa-creative-commons-by fa-2x" aria-hidden="true"></i>
            <i class="fab fa-creative-commons-nc fa-2x" aria-hidden="true"></i>
            <i class="fab fa-creative-commons-nd fa-2x" aria-hidden="true"></i></a></p>
        <p class="powered-by">Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target="_blank"
            rel="noopener">Wowchemy</a> ‚Äî the free, <a href="https://github.com/wowchemy/wowchemy-hugo-themes"
            target="_blank" rel="noopener">open source</a> website builder that empowers creators.</p>
      </footer>
    </div>
  </div>
</body>

</html>